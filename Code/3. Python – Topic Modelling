#3. Topic Modelling – Python (BERTopic)

#STEP 1 - Import new dataset with modularity class
df = pd.read_csv("Nordstream_class_data.csv")

#STEP 2 - Data cleaning
#visualise top 10 communities
df["community_id"].groupby(df['community_id']).value_counts().nlargest(10)

#Assign top communities to distinct datasets
df180 = df[df["community_id"] == 180]
df106 = df[df["community_id"] == 106]
df246 = df[df["community_id"] == 246]
df1029= df[df["community_id"] == 1029]
df271 = df[df["community_id"] == 271]
df479 = df[df["community_id"] == 479]
df771 = df[df["community_id"] == 771]
df232 = df[df["community_id"] == 232]     

#clean up the text data in the DataFrames by removing URLs, Twitter handles, and non-alphabetic characters, and filter out empty tweets.
import re

df1029.text = df1029.apply(lambda row: re.sub(r"http\S+", "", row.text).lower(), 1)
df1029.text = df1029.apply(lambda row: " ".join(filter(lambda x:x[0]!="@", row.text.split())), 1)
df1029.text = df1029.apply(lambda row: " ".join(re.sub("[^a-zA-ZüöäÜÖÄ]+", " ", row.text).split()), 1)
df1029 = df1029.loc[(df1029.text != ""), :]

df106.text = df106.apply(lambda row: re.sub(r"http\S+", "", row.text).lower(), 1)
df106.text = df106.apply(lambda row: " ".join(filter(lambda x:x[0]!="@", row.text.split())), 1)
df106.text = df106.apply(lambda row: " ".join(re.sub("[^a-zA-ZüöäÜÖÄ]+", " ", row.text).split()), 1)
df106 = df106.loc[(df106.text != ""), :]

df180.text = df180.apply(lambda row: re.sub(r"http\S+", "", row.text).lower(), 1)
df180.text = df180.apply(lambda row: " ".join(filter(lambda x:x[0]!="@", row.text.split())), 1)
df180.text = df180.apply(lambda row: " ".join(re.sub("[^a-zA-ZüöäÜÖÄ]+", " ", row.text).split()), 1)
df180 = df180.loc[(df180.text != ""), :]

df246.text = df246.apply(lambda row: re.sub(r"http\S+", "", row.text).lower(), 1)
df246.text = df246.apply(lambda row: " ".join(filter(lambda x:x[0]!="@", row.text.split())), 1)
df246.text = df246.apply(lambda row: " ".join(re.sub("[^a-zA-ZüöäÜÖÄ]+", " ", row.text).split()), 1)
df246 = df246.loc[(df246.text != ""), :]

df271.text = df271.apply(lambda row: re.sub(r"http\S+", "", row.text).lower(), 1)
df271.text = df271.apply(lambda row: " ".join(filter(lambda x:x[0]!="@", row.text.split())), 1)
df271.text = df271.apply(lambda row: " ".join(re.sub("[^a-zA-ZüöäÜÖÄ]+", " ", row.text).split()), 1)
df271 = df271.loc[(df271.text != ""), :]

df479.text = df479.apply(lambda row: re.sub(r"http\S+", "", row.text).lower(), 1)
df479.text = df479.apply(lambda row: " ".join(filter(lambda x:x[0]!="@", row.text.split())), 1)
df479.text = df479.apply(lambda row: " ".join(re.sub("[^a-zA-ZüöäÜÖÄ]+", " ", row.text).split()), 1)
df479 = df479.loc[(df479.text != ""), :]

df771.text = df771.apply(lambda row: re.sub(r"http\S+", "", row.text).lower(), 1)
df771.text = df771.apply(lambda row: " ".join(filter(lambda x:x[0]!="@", row.text.split())), 1)
df771.text = df771.apply(lambda row: " ".join(re.sub("[^a-zA-ZüöäÜÖÄ]+", " ", row.text).split()), 1)
df771 = df771.loc[(df771.text != ""), :]

df232.text = df232.apply(lambda row: re.sub(r"http\S+", "", row.text).lower(), 1)
df232.text = df232.apply(lambda row: " ".join(filter(lambda x:x[0]!="@", row.text.split())), 1)
df232.text = df232.apply(lambda row: " ".join(re.sub("[^a-zA-ZüöäÜÖÄ]+", " ", row.text).split()), 1)
df232 = df232.loc[(df271.text != ""), :]


#STEP 3 - Conduct BERTopic
#3.1 Install bertopic and import libraries

!pip install bertopic

import pandas as pd
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer

from umap import UMAP
from hdbscan import HDBSCAN
from sklearn.feature_extraction.text import CountVectorizer

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stopwords = stopwords.words('english') + stopwords.words("german")

#3.2 Compute and save models on drive for future analysis for the top 5 communities

#Choose models and parameters
model = SentenceTransformer("all-MiniLM-L6-v2")
vectorizer = CountVectorizer(stop_words=stopwords)
umap_model = UMAP(n_neighbors=10, n_components=5, min_dist=0.0, metric='cosine', random_state=42)
hdbscan_model = HDBSCAN(min_cluster_size=800, min_samples=2, metric='euclidean', cluster_selection_method='eom')

#We chose a high cluster size (which we selected it after trying out different sizes) in order to keep the number of topics low. When the topic number was too high, we reduced it. 
#We are interested in the main topics of discussion in each community.

##Community 180
documents = list(df180['text'].str.lower())
document_vectors = model.encode(documents, show_progress_bar=True)

topic_model = BERTopic(
    language = "multilingual"
    embedding_model=model,
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    vectorizer_model=vectorizer,
    verbose=True
).fit(documents, document_vectors)

topic_model.save("Nordstream_com_180_model")	

print("Total Number of Topics:", len(topic_model.get_topic_freq()))

topic_model.reduce_topics(documents, nr_topics=10)

##Community 106
documents = list(df106['text'].str.lower())
document_vectors = model.encode(documents, show_progress_bar=True)

topic_model = BERTopic(
    language = "multilingual",
    embedding_model=model,
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    vectorizer_model=vectorizer,
    verbose=True
).fit(documents, document_vectors)

print("Total Number of Topics:", len(topic_model.get_topic_freq()))

topic_model.save("Nordstream_com_106_model")	

##Community 1029
documents = list(df1029['text'].str.lower())
document_vectors = model.encode(documents, show_progress_bar=True)

topic_model = BERTopic(
    language="multilingual",
    embedding_model=model,
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    vectorizer_model=vectorizer,
    verbose=True
).fit(documents, document_vectors)

topic_model.save("Nordstream_com_1029_model")

print("Total Number of Topics:", len(topic_model.get_topic_freq()))

##Community 246
documents = list(df246['text'].str.lower())
document_vectors = model.encode(documents, show_progress_bar=True)

topic_model = BERTopic(
    language="multilingual",
    embedding_model=model,
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    vectorizer_model=vectorizer,
    verbose=True
).fit(documents, document_vectors)

topic_model.save("Nordstream_com_246_model")	

print("Total Number of Topics:", len(topic_model.get_topic_freq()))

##Community 271
documents = list(df271['text'].str.lower())
document_vectors = model.encode(documents, show_progress_bar=True)

topic_model = BERTopic(
    language="multilingual",
    embedding_model=model,
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    vectorizer_model=vectorizer,
    verbose=True
).fit(documents, document_vectors)

topic_model.save("Nordstream_com_271_model")	

print("Total Number of Topics:", len(topic_model.get_topic_freq()))


#3.3 Compute and save models on drive for future analysis for the next 3 communities
#Given that these communities are much smaller, we decided to reduce the min_cluster_size from 800 to 300 in order to have a similar amout of topics. This choice was made after trial and error, trying out different parameters' setups to see which worked best for our case.
vectorizer = CountVectorizer(stop_words=stopwords)
umap_model = UMAP(n_neighbors=10, n_components=5, min_dist=0.0, metric='cosine', random_state=42)
hdbscan_model = HDBSCAN(min_cluster_size=300, min_samples=2, metric='euclidean', cluster_selection_method='eom')

##Community 771
documents = list(df771['text'].str.lower())
document_vectors = model.encode(documents, show_progress_bar=True)

topic_model = BERTopic(
    language="multilingual",
    embedding_model=model,
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    vectorizer_model=vectorizer,
    verbose=True
).fit(documents, document_vectors)

topic_model.save("Nordstream_com_771_model")	

##Community 232
hdbscan_model = HDBSCAN(min_cluster_size=300, min_samples=2, metric='euclidean', cluster_selection_method='eom')

documents = list(df232['text'].str.lower())
document_vectors = model.encode(documents, show_progress_bar=True)

topic_model = BERTopic(
    language="multilingual",
    embedding_model=model,
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    vectorizer_model=vectorizer,
    verbose=True
).fit(documents, document_vectors)

topic_model.save("Nordstream_com_232_model")	

##Community 479 - here we had to increase the min_cluster_size a bit to 500
documents = list(df479['text'].str.lower())
document_vectors = model.encode(documents, show_progress_bar=True)

topic_model = BERTopic(
    language="multilingual",
    embedding_model=model,
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    vectorizer_model=vectorizer,
    verbose=True
).fit(documents, document_vectors)

topic_model.save("Nordstream_com_479_model")	

##3.4 Reload models for new Colab session (having already mounted the drive as well as installed and imported BERTopic)
model106 = BERTopic.load("Nordstream_com_106_model")	
model246 = BERTopic.load("Nordstream_com_246_model")
model271 = BERTopic.load("Nordstream_com_271_model")
model1029 = BERTopic.load("Nordstream_com_1029_model")
model180 = BERTopic.load("Nordstream_com_246_model")
model232 = BERTopic.load("Nordstream_com_232_model")	
model479 = BERTopic.load("Nordstream_com_479_model")	
model771 = BERTopic.load("Nordstream_com_771_model")	


##3.4 Assign topics back to their original tweets
df1029["topics"] = model1029.topics_
df106["topics"] = model106.topics_
df180["topics"] = model180.topics_
df246["topics"] = model246.topics_
df271["topics"] = model271.topics_
df479["topics"] = model479.topics_
df771["topics"] = model771.topics_
df232["topics"] = model232.topics_

##3.5 To analyse the topic modelling we used the following functions
## Please note that here the analysis of the topic modelling was done through multiple iterations to better get a sense
## of the topics of discussion in the different communities. Here we generalised for a community _n. 


len(model_n.get_topic_freq()) # number of topics

model_n.visualize_topics() # intertopic distance map

model_n.visualize_barchart(n_words=10, top_n_topics = 15) # bar chart

freq = model_n.get_topic_info(); freq.head(20) # terms visualisation

model_n.get_representative_docs() # representative tweets for the topics

list(df_n.text[df_n["topics"] == x].sample(10)) #where "x" is the topic number to be visualised
# this was used to access samples of tweets in the dataset related to topic x in community n

#–-----------------------------------------------

# EXTRA
# For the first 5 communities, we also decided to run LDA topic modelling to see if the results would hold.

#Install and import necessary libraries

import locale
locale.getpreferredencoding = lambda: "UTF-8"
!pip install textacy

import textacy
import textacy.tm
from functools import partial


####Com 1029
  #####Train
documents = list(df1029['text'].str.lower())

corpus = textacy.Corpus("en_core_web_sm", documents)
docs_terms = (textacy.extract.terms(doc,ngs=partial(textacy.extract.ngrams, n=1, include_pos={"NOUN", "ADJ"}),ents=partial(textacy.extract.entities, include_types={ "ORG", "GPE", "LOC"}))for doc in corpus)

tokenized_docs = (textacy.extract.terms_to_strings(doc_terms, by="lemma") for doc_terms in docs_terms)
doc_term_matrix, vocab = textacy.representations.build_doc_term_matrix(tokenized_docs,tf_type="linear", idf_type="smooth")
id_to_term = {id_: term for term, id_ in vocab.items()}

model = textacy.tm.TopicModel("lda", n_topics=15)
model.fit(doc_term_matrix)
doc_topic_matrix = model.transform(doc_term_matrix)

for topic_idx, terms in model.top_topic_terms(id_to_term, top_n=10):
    print(f"topic {topic_idx}: {'   '.join(terms)}")

  #####Visualisation
_ = model.termite_plot(doc_term_matrix, id_to_term, n_terms=30, highlight_topics=[1,3,10,11,12])

####Com 106
  #####Train 
documents = list(df106['text'].str.lower())

corpus = textacy.Corpus("en_core_web_sm", documents)
docs_terms = (textacy.extract.terms(doc,ngs=partial(textacy.extract.ngrams, n=1, include_pos={"NOUN", "ADJ"}),ents=partial(textacy.extract.entities, include_types={ "ORG", "GPE", "LOC"}))for doc in corpus)

tokenized_docs = (textacy.extract.terms_to_strings(doc_terms, by="lemma") for doc_terms in docs_terms)
doc_term_matrix, vocab = textacy.representations.build_doc_term_matrix(tokenized_docs,tf_type="linear", idf_type="smooth")
id_to_term = {id_: term for term, id_ in vocab.items()}

model = textacy.tm.TopicModel("lda", n_topics=15)
model.fit(doc_term_matrix)
doc_topic_matrix = model.transform(doc_term_matrix)

for topic_idx, terms in model.top_topic_terms(id_to_term, top_n=10):
    print(f"topic {topic_idx}: {'   '.join(terms)}")

  #####Visualisation
_ = model.termite_plot(doc_term_matrix, id_to_term, n_terms=30, highlight_topics=[6,8,10,11,13,14])

####Com 180
  #####Train

documents = list(df180['text'].str.lower())

corpus = textacy.Corpus("en_core_web_sm", documents)
docs_terms = (textacy.extract.terms(doc,ngs=partial(textacy.extract.ngrams, n=1, include_pos={"NOUN", "ADJ"}),ents=partial(textacy.extract.entities, include_types={ "ORG", "GPE", "LOC"}))for doc in corpus)

tokenized_docs = (textacy.extract.terms_to_strings(doc_terms, by="lemma") for doc_terms in docs_terms)
doc_term_matrix, vocab = textacy.representations.build_doc_term_matrix(tokenized_docs,tf_type="linear", idf_type="smooth")
id_to_term = {id_: term for term, id_ in vocab.items()}

model = textacy.tm.TopicModel("lda", n_topics=15)
model.fit(doc_term_matrix)
doc_topic_matrix = model.transform(doc_term_matrix)

for topic_idx, terms in model.top_topic_terms(id_to_term, top_n=10):
    print(f"topic {topic_idx}: {'   '.join(terms)}")

  #####Visaulisation
_ = model.termite_plot(doc_term_matrix, id_to_term, n_terms=30, highlight_topics=[4,5,9,13])

####Com246
  #####Train
documents = list(df246['text'].str.lower())

corpus = textacy.Corpus("en_core_web_sm", documents)
docs_terms = (textacy.extract.terms(doc,ngs=partial(textacy.extract.ngrams, n=1, include_pos={"NOUN", "ADJ"}),ents=partial(textacy.extract.entities, include_types={ "ORG", "GPE", "LOC"}))for doc in corpus)

tokenized_docs = (textacy.extract.terms_to_strings(doc_terms, by="lemma") for doc_terms in docs_terms)
doc_term_matrix, vocab = textacy.representations.build_doc_term_matrix(tokenized_docs,tf_type="linear", idf_type="smooth")
id_to_term = {id_: term for term, id_ in vocab.items()}

model = textacy.tm.TopicModel("lda", n_topics=15)
model.fit(doc_term_matrix)
doc_topic_matrix = model.transform(doc_term_matrix)

  ##Vizualisation
_ = model.termite_plot(doc_term_matrix, id_to_term, n_terms=30, highlight_topics=[3,6,10,14])

####Com 271
  #####Train
documents = list(df271['text'].str.lower())

corpus = textacy.Corpus("en_core_web_sm", documents)
docs_terms = (textacy.extract.terms(doc,ngs=partial(textacy.extract.ngrams, n=1, include_pos={"NOUN", "ADJ"}),ents=partial(textacy.extract.entities, include_types={ "ORG", "GPE", "LOC"}))for doc in corpus)

tokenized_docs = (textacy.extract.terms_to_strings(doc_terms, by="lemma") for doc_terms in docs_terms)
doc_term_matrix, vocab = textacy.representations.build_doc_term_matrix(tokenized_docs,tf_type="linear", idf_type="smooth")
id_to_term = {id_: term for term, id_ in vocab.items()}

model = textacy.tm.TopicModel("lda", n_topics=15)
model.fit(doc_term_matrix)
doc_topic_matrix = model.transform(doc_term_matrix)

for topic_idx, terms in model.top_topic_terms(id_to_term, top_n=10):
    print(f"topic {topic_idx}: {'   '.join(terms)}")

  ##Vizsualisation
_ = model.termite_plot(doc_term_matrix, id_to_term, n_terms=30, highlight_topics=[1,2,6,9,11,13])



