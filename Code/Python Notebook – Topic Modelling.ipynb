{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y1IE5ClROfM"
      },
      "source": [
        "# **`Topic Modelling`**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we mount the drive, set the directory, import the dataset with the statistics from Gephi (modularity, page rank, and community number), and create subsets for the top communities."
      ],
      "metadata": {
        "id": "Mzzq4dfQl2Mq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import pandas as pd \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/drive/MyDrive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpvRnAr6A2qT",
        "outputId": "f8221a9b-dd20-4ee9-bfd2-fafdd1f13419"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"NordstreamSabotage_class_data.csv\")\n",
        "df"
      ],
      "metadata": {
        "id": "o0qjVLr5kQ9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbYy0XtNRRAn"
      },
      "outputs": [],
      "source": [
        "df[\"community_id\"].groupby(df['community_id']).value_counts().nlargest(10) # visualise top 10 communities "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SicfsZPPRQ-E"
      },
      "outputs": [],
      "source": [
        "# create subsets for top 5 communities\n",
        "df180 = df[df[\"community_id\"] == 180]\n",
        "df106 = df[df[\"community_id\"] == 106]\n",
        "df246 = df[df[\"community_id\"] == 246]\n",
        "df1029= df[df[\"community_id\"] == 1029]\n",
        "df271 = df[df[\"community_id\"] == 271]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# and for the next 3 communities\n",
        "df479 = df[df[\"community_id\"] == 479]\n",
        "df771 = df[df[\"community_id\"] == 771]\n",
        "df232 = df[df[\"community_id\"] == 232]  "
      ],
      "metadata": {
        "id": "vdZhV-bsfSW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE19WFkMkbX5"
      },
      "source": [
        "## **BERTopic**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Install BERTopic, load libraries, text setup"
      ],
      "metadata": {
        "id": "qnlRLtBvT5hI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8yrHBOCkeDH"
      },
      "outputs": [],
      "source": [
        "!pip install bertopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mLcMpTsZUcV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = stopwords.words('english') + stopwords.words(\"german\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#clean up the text data in the DataFrames by removing URLs, Twitter handles, and non-alphabetic characters, \n",
        "#and filter out empty tweets.\n",
        "import re\n",
        "\n",
        "df1029.text = df1029.apply(lambda row: re.sub(r\"http\\S+\", \"\", row.text).lower(), 1)\n",
        "df1029.text = df1029.apply(lambda row: \" \".join(filter(lambda x:x[0]!=\"@\", row.text.split())), 1)\n",
        "df1029.text = df1029.apply(lambda row: \" \".join(re.sub(\"[^a-zA-ZüöäÜÖÄ]+\", \" \", row.text).split()), 1)\n",
        "df1029 = df1029.loc[(df1029.text != \"\"), :]\n",
        "\n",
        "df106.text = df106.apply(lambda row: re.sub(r\"http\\S+\", \"\", row.text).lower(), 1)\n",
        "df106.text = df106.apply(lambda row: \" \".join(filter(lambda x:x[0]!=\"@\", row.text.split())), 1)\n",
        "df106.text = df106.apply(lambda row: \" \".join(re.sub(\"[^a-zA-ZüöäÜÖÄ]+\", \" \", row.text).split()), 1)\n",
        "df106 = df106.loc[(df106.text != \"\"), :]\n",
        "\n",
        "df180.text = df180.apply(lambda row: re.sub(r\"http\\S+\", \"\", row.text).lower(), 1)\n",
        "df180.text = df180.apply(lambda row: \" \".join(filter(lambda x:x[0]!=\"@\", row.text.split())), 1)\n",
        "df180.text = df180.apply(lambda row: \" \".join(re.sub(\"[^a-zA-ZüöäÜÖÄ]+\", \" \", row.text).split()), 1)\n",
        "df180 = df180.loc[(df180.text != \"\"), :]\n",
        "\n",
        "df246.text = df246.apply(lambda row: re.sub(r\"http\\S+\", \"\", row.text).lower(), 1)\n",
        "df246.text = df246.apply(lambda row: \" \".join(filter(lambda x:x[0]!=\"@\", row.text.split())), 1)\n",
        "df246.text = df246.apply(lambda row: \" \".join(re.sub(\"[^a-zA-ZüöäÜÖÄ]+\", \" \", row.text).split()), 1)\n",
        "df246 = df246.loc[(df246.text != \"\"), :]\n",
        "\n",
        "df271.text = df271.apply(lambda row: re.sub(r\"http\\S+\", \"\", row.text).lower(), 1)\n",
        "df271.text = df271.apply(lambda row: \" \".join(filter(lambda x:x[0]!=\"@\", row.text.split())), 1)\n",
        "df271.text = df271.apply(lambda row: \" \".join(re.sub(\"[^a-zA-ZüöäÜÖÄ]+\", \" \", row.text).split()), 1)\n",
        "df271 = df271.loc[(df271.text != \"\"), :]\n",
        "\n",
        "df479.text = df479.apply(lambda row: re.sub(r\"http\\S+\", \"\", row.text).lower(), 1)\n",
        "df479.text = df479.apply(lambda row: \" \".join(filter(lambda x:x[0]!=\"@\", row.text.split())), 1)\n",
        "df479.text = df479.apply(lambda row: \" \".join(re.sub(\"[^a-zA-ZüöäÜÖÄ]+\", \" \", row.text).split()), 1)\n",
        "df479 = df479.loc[(df479.text != \"\"), :]\n",
        "\n",
        "df771.text = df771.apply(lambda row: re.sub(r\"http\\S+\", \"\", row.text).lower(), 1)\n",
        "df771.text = df771.apply(lambda row: \" \".join(filter(lambda x:x[0]!=\"@\", row.text.split())), 1)\n",
        "df771.text = df771.apply(lambda row: \" \".join(re.sub(\"[^a-zA-ZüöäÜÖÄ]+\", \" \", row.text).split()), 1)\n",
        "df771 = df771.loc[(df771.text != \"\"), :]\n",
        "\n",
        "df232.text = df232.apply(lambda row: re.sub(r\"http\\S+\", \"\", row.text).lower(), 1)\n",
        "df232.text = df232.apply(lambda row: \" \".join(filter(lambda x:x[0]!=\"@\", row.text.split())), 1)\n",
        "df232.text = df232.apply(lambda row: \" \".join(re.sub(\"[^a-zA-ZüöäÜÖÄ]+\", \" \", row.text).split()), 1)\n",
        "df232 = df232.loc[(df271.text != \"\"), :]"
      ],
      "metadata": {
        "id": "dWb8F5INsh82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Compute and save models on drive"
      ],
      "metadata": {
        "id": "rjljP4KdpNVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Top 5 communities"
      ],
      "metadata": {
        "id": "erFtjgMQCkXd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Com 1029"
      ],
      "metadata": {
        "id": "f-g0mTTamgXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = list(df1029['text'].str.lower())"
      ],
      "metadata": {
        "id": "f73vJfa7pUre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "document_vectors = model.encode(documents, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "pvYGE4JhpUom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(stop_words=stopwords)\n",
        "umap_model = UMAP(n_neighbors=10, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=800, min_samples=2, metric='euclidean', cluster_selection_method='eom')"
      ],
      "metadata": {
        "id": "tuuhLzSZpUmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model = BERTopic(\n",
        "    language=\"multilingual\",\n",
        "    embedding_model=model,\n",
        "    umap_model=umap_model,\n",
        "    hdbscan_model=hdbscan_model,\n",
        "    vectorizer_model=vectorizer,\n",
        "    verbose=True\n",
        ").fit(documents, document_vectors)"
      ],
      "metadata": {
        "id": "zfyzkyKNpUjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.save(\"Nordstream_com_1029_model\")"
      ],
      "metadata": {
        "id": "KBTnnWj_mVvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total Number of Topics:\", len(topic_model.get_topic_freq()))"
      ],
      "metadata": {
        "id": "YxxYyU28-rM-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daf3cdad-6f2b-4e4e-aeb3-14e5028c9866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Number of Topics: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Com 106"
      ],
      "metadata": {
        "id": "bPO4xo0Imk25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = list(df106['text'].str.lower())"
      ],
      "metadata": {
        "id": "2nVbQ2Humnyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "document_vectors = model.encode(documents, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "mjtrCZt8moRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(stop_words=stopwords)\n",
        "umap_model = UMAP(n_neighbors=10, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=800, min_samples=2, metric='euclidean', cluster_selection_method='eom')"
      ],
      "metadata": {
        "id": "nAlkOTHSmoEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model = BERTopic(\n",
        "    language = \"multilingual\",\n",
        "    embedding_model=model,\n",
        "    umap_model=umap_model,\n",
        "    hdbscan_model=hdbscan_model,\n",
        "    vectorizer_model=vectorizer,\n",
        "    verbose=True\n",
        ").fit(documents, document_vectors)"
      ],
      "metadata": {
        "id": "yVQ4M9k8mn-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.save(\"Nordstream_com_106_model\")\t"
      ],
      "metadata": {
        "id": "xPVY6iElqN1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total Number of Topics:\", len(topic_model.get_topic_freq()))"
      ],
      "metadata": {
        "id": "4luS1YB6nuKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Com 180"
      ],
      "metadata": {
        "id": "owrcnza_mo03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = list(df180['text'].str.lower())"
      ],
      "metadata": {
        "id": "t2oFBV00n7Pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "document_vectors = model.encode(documents, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "tIZtAoKMn7Pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(stop_words=stopwords)\n",
        "umap_model = UMAP(n_neighbors=10, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=800, min_samples=2, metric='euclidean', cluster_selection_method='eom')"
      ],
      "metadata": {
        "id": "0-AiQM74n7Pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model = BERTopic(\n",
        "    language = \"multilingual\"\n",
        "    embedding_model=model,\n",
        "    umap_model=umap_model,\n",
        "    hdbscan_model=hdbscan_model,\n",
        "    vectorizer_model=vectorizer,\n",
        "    verbose=True\n",
        ").fit(documents, document_vectors)"
      ],
      "metadata": {
        "id": "r_zBLFhLn7Pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.save(\"Nordstream_com_180_model\")\t\t"
      ],
      "metadata": {
        "id": "TDxntEjPyhRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total Number of Topics:\", len(topic_model.get_topic_freq()))"
      ],
      "metadata": {
        "id": "UXP8ybNcn7Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Com 246"
      ],
      "metadata": {
        "id": "uayD9ACrmtnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = list(df246['text'].str.lower())"
      ],
      "metadata": {
        "id": "l0yFOwjln8RV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "document_vectors246 = model.encode(documents, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "6VeaMPz0n8RX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(stop_words=stopwords)\n",
        "umap_model = UMAP(n_neighbors=10, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=800, min_samples=2, metric='euclidean', cluster_selection_method='eom')"
      ],
      "metadata": {
        "id": "LfgeXvjYn8RX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model = BERTopic(\n",
        "    language = \"multilingual\",\n",
        "    embedding_model=model,\n",
        "    umap_model=umap_model,\n",
        "    hdbscan_model=hdbscan_model,\n",
        "    vectorizer_model=vectorizer,\n",
        "    verbose=True\n",
        ").fit(documents, document_vectors246)"
      ],
      "metadata": {
        "id": "mz998Xc7n8RY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.save(\"Nordstream_com_246_model\")\t"
      ],
      "metadata": {
        "id": "u0PjqRYoZIv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total Number of Topics:\", len(topic_model.get_topic_freq()))"
      ],
      "metadata": {
        "id": "-HdfzSVNn8RY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Com 271"
      ],
      "metadata": {
        "id": "1ig7tcMCmyMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = list(df271['text'].str.lower())"
      ],
      "metadata": {
        "id": "OSw3S1qLn9So"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "document_vectors271 = model.encode(documents, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "LNyO-HL3n9Sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(stop_words=stopwords)\n",
        "umap_model = UMAP(n_neighbors=10, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=800, min_samples=2, metric='euclidean', cluster_selection_method='eom')"
      ],
      "metadata": {
        "id": "HgtYL6dVn9Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model = BERTopic(\n",
        "    language = \"multilingual\",\n",
        "    embedding_model=model,\n",
        "    umap_model=umap_model,\n",
        "    hdbscan_model=hdbscan_model,\n",
        "    vectorizer_model=vectorizer,\n",
        "    verbose=True\n",
        ").fit(documents, document_vectors271)"
      ],
      "metadata": {
        "id": "76NowADYn9Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.save(\"Nordstream_com_271_model\")\t"
      ],
      "metadata": {
        "id": "Byp4Y_uwhI8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total Number of Topics:\", len(topic_model.get_topic_freq()))"
      ],
      "metadata": {
        "id": "CanYdR5Gn9Ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Secondary 3 communities"
      ],
      "metadata": {
        "id": "geoTvyFg856w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given that these communities are much smaller, we decided to reduce the min_cluster_size from 800 to 500 in order to have a similar amout of topics. This choice was made after trial and error, trying out different parameters' setups to see which worked best for our case."
      ],
      "metadata": {
        "id": "j36dLCQR-VAr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Com 771"
      ],
      "metadata": {
        "id": "3J1LaWAx8-l1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = list(df771['text'].str.lower())"
      ],
      "metadata": {
        "id": "5uAX02y69bJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "document_vectors = model.encode(documents, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "R1sDRfXX9bJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(stop_words=stopwords)\n",
        "umap_model = UMAP(n_neighbors=10, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=300, min_samples=2, metric='euclidean', cluster_selection_method='eom')"
      ],
      "metadata": {
        "id": "hn8gKfiZ9bJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model = BERTopic(\n",
        "    language=\"multilingual\",\n",
        "    embedding_model=model,\n",
        "    umap_model=umap_model,\n",
        "    hdbscan_model=hdbscan_model,\n",
        "    vectorizer_model=vectorizer,\n",
        "    verbose=True\n",
        ").fit(documents, document_vectors)"
      ],
      "metadata": {
        "id": "cPMMH_8K9bJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.save(\"Nordstream_com_771_model\")\t"
      ],
      "metadata": {
        "id": "BgbJp5-X9bJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Com 479"
      ],
      "metadata": {
        "id": "19YOLCab9BSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = list(df479['text'].str.lower())"
      ],
      "metadata": {
        "id": "zOggGIYT9cil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "document_vectors = model.encode(documents, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "JKr_6r9B9cim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(stop_words=stopwords)\n",
        "umap_model = UMAP(n_neighbors=10, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=500, min_samples=2, metric='euclidean', cluster_selection_method='eom')"
      ],
      "metadata": {
        "id": "lEb_xru99cim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model = BERTopic(\n",
        "    language = \"multilingual\",\n",
        "    embedding_model=model,\n",
        "    umap_model=umap_model,\n",
        "    hdbscan_model=hdbscan_model,\n",
        "    vectorizer_model=vectorizer,\n",
        "    verbose=True\n",
        ").fit(documents, document_vectors)"
      ],
      "metadata": {
        "id": "WbJYlXoS9cim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.save(\"Nordstream_com_479_model\")\t"
      ],
      "metadata": {
        "id": "nWua1cC69cim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Com 232"
      ],
      "metadata": {
        "id": "s0QS0i6Q9Lre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = list(df232['text'].str.lower())"
      ],
      "metadata": {
        "id": "xE-WHKyn9dAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "document_vectors = model.encode(documents, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "cuN7pjP49dAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(stop_words=stopwords)\n",
        "umap_model = UMAP(n_neighbors=10, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=300, min_samples=2, metric='euclidean', cluster_selection_method='eom')"
      ],
      "metadata": {
        "id": "RJTmQa6T9dAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model = BERTopic(\n",
        "    language = \"multilingual\",\n",
        "    embedding_model=model,\n",
        "    umap_model=umap_model,\n",
        "    hdbscan_model=hdbscan_model,\n",
        "    vectorizer_model=vectorizer,\n",
        "    verbose=True\n",
        ").fit(documents, document_vectors)"
      ],
      "metadata": {
        "id": "_PSQ-8209dAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.save(\"Nordstream_com_232_model\")\t"
      ],
      "metadata": {
        "id": "ul2pcZbW9dAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Load existing Models for analysis"
      ],
      "metadata": {
        "id": "cidQo2EyndAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TOP 5 COMMUNITIES\n",
        "model106 = BERTopic.load(\"Nordstream_com_106_model\")\t\n",
        "model246 = BERTopic.load(\"Nordstream_com_246_model\")\n",
        "model271 = BERTopic.load(\"Nordstream_com_271_model\")\n",
        "model1029 = BERTopic.load(\"Nordstream_com_1029_model\")\n",
        "model180 = BERTopic.load(\"Nordstream_com_246_model\")"
      ],
      "metadata": {
        "id": "H5SeB4F2ncYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SECONDARY 3 COMMUNITIES\n",
        "model232 = BERTopic.load(\"Nordstream_com_232_model\")\t\n",
        "model479 = BERTopic.load(\"Nordstream_com_479_model\")\t\n",
        "model771 = BERTopic.load(\"Nordstream_com_771_model\")\t\n"
      ],
      "metadata": {
        "id": "Oandwve7W_Zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Interpret and analyse the BERTopic models"
      ],
      "metadata": {
        "id": "0Zj_XjbwngjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assign topics back to their original tweets\n",
        "df1029[\"topics\"] = model1029.topics_\n",
        "df106[\"topics\"] = model106.topics_\n",
        "df180[\"topics\"] = model1029.topics_\n",
        "df246[\"topics\"] = model246.topics_\n",
        "df271[\"topics\"] = model271.topics_\n",
        "df232[\"topics\"] = model271.topics_\n",
        "df771[\"topics\"] = model271.topics_\n",
        "df479[\"topics\"] = model271.topics_"
      ],
      "metadata": {
        "id": "o7BOzYhGDCJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To analyse the topics we used the same functions for all communities. We decided to leave the functions here below only for community 106 but not to report the for all communities. This decision is because of the fact that the investigation was quite dynamic in looking into each topics by altering the code in the same cells rather than copying paste the code indefinitely."
      ],
      "metadata": {
        "id": "F4dFT9LOn5kl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(model106.get_topic_freq()) # number of topics"
      ],
      "metadata": {
        "id": "1VOiTwkSn5CN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model106.visualize_topics() # intertopic distance map"
      ],
      "metadata": {
        "id": "6INSjouQoneW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model106.visualize_barchart(n_words=10, top_n_topics = 15) # terms bar chart"
      ],
      "metadata": {
        "id": "E4asjmHvoq4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq = model106.get_topic_info(); freq.head(20) # terms visualisation"
      ],
      "metadata": {
        "id": "PocvVPzhotoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model106.get_representative_docs() # representative tweets for the topics"
      ],
      "metadata": {
        "id": "9X54gvw2ou_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(df106.text[df106[\"topics\"] == x].sample(10)) # where \"x\" is the topic number to be visualised\n",
        "# this was used to access samples of tweets in the dataset related to topic x in community n"
      ],
      "metadata": {
        "id": "k_mG8XvNoloW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df106[df106.user_username == \"username of interest\"] \n",
        "# this was used to investigate tweets from different users who were quite prevalent in some topics "
      ],
      "metadata": {
        "id": "YkgdVneDo5Pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df106.loc[df106['text'].str.contains(\"insert string of interest\")]\n",
        "# this was used to identify lines containing certain words or sequences of words of interest"
      ],
      "metadata": {
        "id": "x-1xp00HpMDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------------------------------------------------------------------------------------------------\n",
        "# **Further Analyses**"
      ],
      "metadata": {
        "id": "Wy5iDLNg_7D9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALR9NuWLRJiS"
      },
      "source": [
        "## **LDA Topic Modelling**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the top 5 communities we also decided to run LDA topic modelling to compare the results."
      ],
      "metadata": {
        "id": "9zZ-CyqiqkJQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7wFyW3ZDFZD"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "# this first two lines were needed to fix an error we got in installing textacy\n",
        "\n",
        "! pip install textacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gi5tKzY-VyT4"
      },
      "outputs": [],
      "source": [
        "import textacy\n",
        "import textacy.tm\n",
        "from functools import partial"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Com 1029"
      ],
      "metadata": {
        "id": "Ny5-ZyBNyJxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Train"
      ],
      "metadata": {
        "id": "Y9ZGfcveZjRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = list(df1029['text'].str.lower())"
      ],
      "metadata": {
        "id": "s_gBKp5Syb3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = textacy.Corpus(\"en_core_web_sm\", documents)\n",
        "docs_terms = (textacy.extract.terms(doc,ngs=partial(textacy.extract.ngrams, n=1, include_pos={\"NOUN\", \"ADJ\"}),ents=partial(textacy.extract.entities, include_types={ \"ORG\", \"GPE\", \"LOC\"}))for doc in corpus)\n",
        "\n",
        "tokenized_docs = (textacy.extract.terms_to_strings(doc_terms, by=\"lemma\") for doc_terms in docs_terms)\n",
        "doc_term_matrix, vocab = textacy.representations.build_doc_term_matrix(tokenized_docs,tf_type=\"linear\", idf_type=\"smooth\")\n",
        "id_to_term = {id_: term for term, id_ in vocab.items()}"
      ],
      "metadata": {
        "id": "Cw5etWZ4yb0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = textacy.tm.TopicModel(\"lda\", n_topics=15)\n",
        "model.fit(doc_term_matrix)\n",
        "doc_topic_matrix = model.transform(doc_term_matrix)"
      ],
      "metadata": {
        "id": "NCRuCgD4ybxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for topic_idx, terms in model.top_topic_terms(id_to_term, top_n=10):\n",
        "    print(f\"topic {topic_idx}: {'   '.join(terms)}\")"
      ],
      "metadata": {
        "id": "WblW4jgmZ-RF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Visualisation"
      ],
      "metadata": {
        "id": "NGzwt_BwZhDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_ = model.termite_plot(doc_term_matrix, id_to_term, n_terms=30, highlight_topics=[1,3,10,11,12])"
      ],
      "metadata": {
        "id": "Q4M4G592Zfbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Com 106"
      ],
      "metadata": {
        "id": "mD-jdNY8yHYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Train "
      ],
      "metadata": {
        "id": "enG1sGRazNFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = list(df106['text'].str.lower())"
      ],
      "metadata": {
        "id": "XlwT1dqpya-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = textacy.Corpus(\"en_core_web_sm\", documents)\n",
        "docs_terms = (textacy.extract.terms(doc,ngs=partial(textacy.extract.ngrams, n=1, include_pos={\"NOUN\", \"ADJ\"}),ents=partial(textacy.extract.entities, include_types={ \"ORG\", \"GPE\", \"LOC\"}))for doc in corpus)\n",
        "\n",
        "tokenized_docs = (textacy.extract.terms_to_strings(doc_terms, by=\"lemma\") for doc_terms in docs_terms)\n",
        "doc_term_matrix, vocab = textacy.representations.build_doc_term_matrix(tokenized_docs,tf_type=\"linear\", idf_type=\"smooth\")\n",
        "id_to_term = {id_: term for term, id_ in vocab.items()}"
      ],
      "metadata": {
        "id": "r10wgCYRya7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = textacy.tm.TopicModel(\"lda\", n_topics=15)\n",
        "model.fit(doc_term_matrix)\n",
        "doc_topic_matrix = model.transform(doc_term_matrix)"
      ],
      "metadata": {
        "id": "Pi9-T-hrya42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for topic_idx, terms in model.top_topic_terms(id_to_term, top_n=10):\n",
        "    print(f\"topic {topic_idx}: {'   '.join(terms)}\")"
      ],
      "metadata": {
        "id": "H8Euzxf4aTvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Visualisation"
      ],
      "metadata": {
        "id": "NX479NfyzPGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_ = model.termite_plot(doc_term_matrix, id_to_term, n_terms=30, highlight_topics=[6,8,10,11,13,14])"
      ],
      "metadata": {
        "id": "O6jfaEgSya2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Com 180\n"
      ],
      "metadata": {
        "id": "PQhTz1wIyBzb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Train"
      ],
      "metadata": {
        "id": "pfHT7HdMzJGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = list(df180['text'].str.lower())"
      ],
      "metadata": {
        "id": "hM3IbJ5tyZOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = textacy.Corpus(\"en_core_web_sm\", documents)\n",
        "docs_terms = (textacy.extract.terms(doc,ngs=partial(textacy.extract.ngrams, n=1, include_pos={\"NOUN\", \"ADJ\"}),ents=partial(textacy.extract.entities, include_types={ \"ORG\", \"GPE\", \"LOC\"}))for doc in corpus)\n",
        "\n",
        "tokenized_docs = (textacy.extract.terms_to_strings(doc_terms, by=\"lemma\") for doc_terms in docs_terms)\n",
        "doc_term_matrix, vocab = textacy.representations.build_doc_term_matrix(tokenized_docs,tf_type=\"linear\", idf_type=\"smooth\")\n",
        "id_to_term = {id_: term for term, id_ in vocab.items()}"
      ],
      "metadata": {
        "id": "jZSLR8JZyZKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = textacy.tm.TopicModel(\"lda\", n_topics=15)\n",
        "model.fit(doc_term_matrix)\n",
        "doc_topic_matrix = model.transform(doc_term_matrix)"
      ],
      "metadata": {
        "id": "z5D3OzwJyZHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for topic_idx, terms in model.top_topic_terms(id_to_term, top_n=10):\n",
        "    print(f\"topic {topic_idx}: {'   '.join(terms)}\")"
      ],
      "metadata": {
        "id": "ndmJ2zN_d5QQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Visaulisation"
      ],
      "metadata": {
        "id": "Mfw-t0V_zKiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_ = model.termite_plot(doc_term_matrix, id_to_term, n_terms=30, highlight_topics=[4,5,9,13])"
      ],
      "metadata": {
        "id": "C0YDL2tKyZEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Com246\n"
      ],
      "metadata": {
        "id": "7Klnww2AxCat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Train"
      ],
      "metadata": {
        "id": "zWd73Bp_y3_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = list(df246['text'].str.lower())"
      ],
      "metadata": {
        "id": "y6buNn1QxHpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = textacy.Corpus(\"en_core_web_sm\", documents)\n",
        "docs_terms = (textacy.extract.terms(doc,ngs=partial(textacy.extract.ngrams, n=1, include_pos={\"NOUN\", \"ADJ\"}),ents=partial(textacy.extract.entities, include_types={ \"ORG\", \"GPE\", \"LOC\"}))for doc in corpus)\n",
        "\n",
        "tokenized_docs = (textacy.extract.terms_to_strings(doc_terms, by=\"lemma\") for doc_terms in docs_terms)\n",
        "doc_term_matrix, vocab = textacy.representations.build_doc_term_matrix(tokenized_docs,tf_type=\"linear\", idf_type=\"smooth\")\n",
        "id_to_term = {id_: term for term, id_ in vocab.items()}"
      ],
      "metadata": {
        "id": "6zCbifYsxIYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = textacy.tm.TopicModel(\"lda\", n_topics=15)\n",
        "model.fit(doc_term_matrix)\n",
        "doc_topic_matrix = model.transform(doc_term_matrix)"
      ],
      "metadata": {
        "id": "RK3oQKE0xISx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for topic_idx, terms in model.top_topic_terms(id_to_term, top_n=10):\n",
        "    print(f\"topic {topic_idx}: {'   '.join(terms)}\")"
      ],
      "metadata": {
        "id": "upne0ZSTEld3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Vizualisation"
      ],
      "metadata": {
        "id": "w8rau3T7y5vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_ = model.termite_plot(doc_term_matrix, id_to_term, n_terms=30, highlight_topics=[3,6,10,14])"
      ],
      "metadata": {
        "id": "5W9Msqz5xIM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Com 271"
      ],
      "metadata": {
        "id": "cJ4MvQeHw_Wo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Train"
      ],
      "metadata": {
        "id": "sGvHh346y9z9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xqf8LowUPDf"
      },
      "outputs": [],
      "source": [
        "documents = list(df271['text'].str.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uj3vzSp6SPkV"
      },
      "outputs": [],
      "source": [
        "corpus = textacy.Corpus(\"en_core_web_sm\", documents)\n",
        "docs_terms = (textacy.extract.terms(doc,ngs=partial(textacy.extract.ngrams, n=1, include_pos={\"NOUN\", \"ADJ\"}),ents=partial(textacy.extract.entities, include_types={ \"ORG\", \"GPE\", \"LOC\"}))for doc in corpus)\n",
        "\n",
        "tokenized_docs = (textacy.extract.terms_to_strings(doc_terms, by=\"lemma\") for doc_terms in docs_terms)\n",
        "doc_term_matrix, vocab = textacy.representations.build_doc_term_matrix(tokenized_docs,tf_type=\"linear\", idf_type=\"smooth\")\n",
        "id_to_term = {id_: term for term, id_ in vocab.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Kqqp4qhSPiP"
      },
      "outputs": [],
      "source": [
        "model = textacy.tm.TopicModel(\"lda\", n_topics=15)\n",
        "model.fit(doc_term_matrix)\n",
        "doc_topic_matrix = model.transform(doc_term_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omxWAdDLSPfg"
      },
      "outputs": [],
      "source": [
        "for topic_idx, terms in model.top_topic_terms(id_to_term, top_n=10):\n",
        "    print(f\"topic {topic_idx}: {'   '.join(terms)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Vizsualisation"
      ],
      "metadata": {
        "id": "NmR_JcnqzC5u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-BPT9MHa6J-"
      },
      "outputs": [],
      "source": [
        "_ = model.termite_plot(doc_term_matrix, id_to_term, n_terms=30, highlight_topics=[1,2,6,9,11,13])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Official figures' presence in the Dataset\n"
      ],
      "metadata": {
        "id": "ds0_i1kApiRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this last section we looked at which of the main politicians were in the dataset."
      ],
      "metadata": {
        "id": "OPf6Kyw2qJ5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Create the politicians' df with the most prevalent figures in the official debate around Nord Stream"
      ],
      "metadata": {
        "id": "_Kwmi7KuA7oQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dic = {\"name\" : [\"Ursula von der Leyen\",\"Joe Biden\",\"Magdalena Anderson\",\"Mateusz Marawiecki\",\n",
        "        \"Dmitri Peskov\",\"Nancy Faeser\",\"Mette Frederiksen\",\"Olaf Scholz\",\"Radek Sikorski\",\n",
        "        \"Sergei Ladrov\",\"Sergei Ryabkov\",\"Vladimr Putin\",\"Jens Stoltenberg\", \"Jonas Gahr Støre\"],\n",
        "       \"country\" : [\"EU\",\"US\",\"Sweden\",\"Poland\",\"Russia\",\"Germany\",\"Denmark\",\"Germany\",\"Poland (MEP)\",\"Russia\",\n",
        "           \"Russia\",\"Russia\",\"NATO\", \"Norway\"],\n",
        "       \"twitter_handles\" : [\"vonderleyen\",\"JoeBiden\",\"INokkvi\",\"MorawieckiM\",\"\",\"NancyFaeser\",\"\",\n",
        "                   \"OlafScholz\",\"adeksikorski\",\"\",\"\",\"\",\"jensstoltenberg\", \"jonasgahrstore\"]}\n",
        "df_politicians = pd.DataFrame(dic)"
      ],
      "metadata": {
        "id": "7Xd8Fk23pnga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Check if they are in one of the two datasets"
      ],
      "metadata": {
        "id": "u9Fp8uwFBHRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pol_tweets = df[df.user_username.isin(df_politicians[\"twitter_handles\"])]\n",
        "df_pol_tweets"
      ],
      "metadata": {
        "id": "z4cC7nhTpndj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####From this section we can conclude that  **only three of the main politicians** involved in the story have used the #NordStream. "
      ],
      "metadata": {
        "id": "vftqs91E_6IR"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "2y1IE5ClROfM",
        "xE19WFkMkbX5",
        "qnlRLtBvT5hI",
        "erFtjgMQCkXd",
        "f-g0mTTamgXA",
        "bPO4xo0Imk25",
        "owrcnza_mo03",
        "uayD9ACrmtnY",
        "1ig7tcMCmyMm",
        "geoTvyFg856w",
        "3J1LaWAx8-l1",
        "19YOLCab9BSF",
        "s0QS0i6Q9Lre",
        "cidQo2EyndAN",
        "0Zj_XjbwngjH",
        "Wy5iDLNg_7D9",
        "ALR9NuWLRJiS",
        "NGzwt_BwZhDT",
        "Mfw-t0V_zKiR",
        "w8rau3T7y5vi",
        "NmR_JcnqzC5u",
        "ds0_i1kApiRz"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}