#STEP 1 - Load the data

from google.colab import drive
drive.mount('/content/drive')

import os 
import pandas as pd 

os.chdir("/content/drive/MyDrive/Colab Notebooks/DDPS/Data Project")
df = pd.read_csv("20230312_Nordstream_df.csv")

#STEP 2 - Subset only german and english tweets
df = pd.concat([df[df["sourcetweet_lang"] == "en"], df[df["sourcetweet_lang"] == "de"]])

#STEP 3 - Construct retweet network and save gml file
! pip install igraph
import igraph

network = igraph.Graph.TupleList([(row['author_id'], 
                                   row['sourcetweet_author_id']) for index, row in df.iterrows() if row['retweet_count'] > 0 and not pd.isna(row['sourcetweet_author_id'])], directed=True)              
igraph.summary(network)
igraph.write(network.simplify(), "nordstream.gml", format="gml")


#STEP 4 - Import gml file on Gephi to visualise graph
#STEP 5 - Calculate modularity class and page rank in gephi


#STEP 6 - Import new dataset with modularity class
df = pd.read_csv("Nordstream_class_data.csv")

#STEP 7 - Data cleaning
#visualise top 10 communities
df["community_id"].groupby(df['community_id']).value_counts().nlargest(10)

#Assign top communities to distinct datasets
df180 = df[df["community_id"] == 180]
df106 = df[df["community_id"] == 106]
df246 = df[df["community_id"] == 246]
df1029= df[df["community_id"] == 1029]
df271 = df[df["community_id"] == 271]
df479 = df[df["community_id"] == 479]
df771 = df[df["community_id"] == 771]
df232 = df[df["community_id"] == 232]     

#clean up the text data in the DataFrames by removing URLs, Twitter handles, and non-alphabetic characters, and filter out empty tweets.
import re

df1029.text = df1029.apply(lambda row: re.sub(r"http\S+", "", row.text).lower(), 1)
df1029.text = df1029.apply(lambda row: " ".join(filter(lambda x:x[0]!="@", row.text.split())), 1)
df1029.text = df1029.apply(lambda row: " ".join(re.sub("[^a-zA-ZüöäÜÖÄ]+", " ", row.text).split()), 1)
df1029 = df1029.loc[(df1029.text != ""), :]

df106.text = df106.apply(lambda row: re.sub(r"http\S+", "", row.text).lower(), 1)
df106.text = df106.apply(lambda row: " ".join(filter(lambda x:x[0]!="@", row.text.split())), 1)
df106.text = df106.apply(lambda row: " ".join(re.sub("[^a-zA-ZüöäÜÖÄ]+", " ", row.text).split()), 1)
df106 = df106.loc[(df106.text != ""), :]

df180.text = df180.apply(lambda row: re.sub(r"http\S+", "", row.text).lower(), 1)
df180.text = df180.apply(lambda row: " ".join(filter(lambda x:x[0]!="@", row.text.split())), 1)
df180.text = df180.apply(lambda row: " ".join(re.sub("[^a-zA-ZüöäÜÖÄ]+", " ", row.text).split()), 1)
df180 = df180.loc[(df180.text != ""), :]

df246.text = df246.apply(lambda row: re.sub(r"http\S+", "", row.text).lower(), 1)
df246.text = df246.apply(lambda row: " ".join(filter(lambda x:x[0]!="@", row.text.split())), 1)
df246.text = df246.apply(lambda row: " ".join(re.sub("[^a-zA-ZüöäÜÖÄ]+", " ", row.text).split()), 1)
df246 = df246.loc[(df246.text != ""), :]

df271.text = df271.apply(lambda row: re.sub(r"http\S+", "", row.text).lower(), 1)
df271.text = df271.apply(lambda row: " ".join(filter(lambda x:x[0]!="@", row.text.split())), 1)
df271.text = df271.apply(lambda row: " ".join(re.sub("[^a-zA-ZüöäÜÖÄ]+", " ", row.text).split()), 1)
df271 = df271.loc[(df271.text != ""), :]

df479.text = df479.apply(lambda row: re.sub(r"http\S+", "", row.text).lower(), 1)
df479.text = df479.apply(lambda row: " ".join(filter(lambda x:x[0]!="@", row.text.split())), 1)
df479.text = df479.apply(lambda row: " ".join(re.sub("[^a-zA-ZüöäÜÖÄ]+", " ", row.text).split()), 1)
df479 = df479.loc[(df479.text != ""), :]

df771.text = df771.apply(lambda row: re.sub(r"http\S+", "", row.text).lower(), 1)
df771.text = df771.apply(lambda row: " ".join(filter(lambda x:x[0]!="@", row.text.split())), 1)
df771.text = df771.apply(lambda row: " ".join(re.sub("[^a-zA-ZüöäÜÖÄ]+", " ", row.text).split()), 1)
df771 = df771.loc[(df771.text != ""), :]

df232.text = df232.apply(lambda row: re.sub(r"http\S+", "", row.text).lower(), 1)
df232.text = df232.apply(lambda row: " ".join(filter(lambda x:x[0]!="@", row.text.split())), 1)
df232.text = df232.apply(lambda row: " ".join(re.sub("[^a-zA-ZüöäÜÖÄ]+", " ", row.text).split()), 1)
df232 = df232.loc[(df271.text != ""), :]


#STEP 8 - Conduct BERTopic
#8.1 Install bertopic and import libraries

!pip install bertopic

import pandas as pd
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer

from umap import UMAP
from hdbscan import HDBSCAN
from sklearn.feature_extraction.text import CountVectorizer

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stopwords = stopwords.words('english') + stopwords.words("german")

#8.2 Compute and save models on drive for future analysis for the top 5 communities

#Choose models and parameters
model = SentenceTransformer("all-MiniLM-L6-v2")
vectorizer = CountVectorizer(stop_words=stopwords)
umap_model = UMAP(n_neighbors=10, n_components=5, min_dist=0.0, metric='cosine', random_state=42)
hdbscan_model = HDBSCAN(min_cluster_size=800, min_samples=2, metric='euclidean', cluster_selection_method='eom')

#We chose a high cluster size (which we selected it after trying out different sizes) in order to keep the number of topics low. When the topic number was too high, we reduced it. 
#We are interested in the main topics of discussion in each community.

##Community 180
documents = list(df180['text'].str.lower())
document_vectors = model.encode(documents, show_progress_bar=True)

topic_model = BERTopic(
    language = "multilingual"
    embedding_model=model,
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    vectorizer_model=vectorizer,
    verbose=True
).fit(documents, document_vectors)

topic_model.save("Nordstream_com_180_model_clussiz_800")	

print("Total Number of Topics:", len(topic_model.get_topic_freq()))

topic_model.reduce_topics(documents, nr_topics=10)

##Community 106
documents = list(df106['text'].str.lower())
document_vectors = model.encode(documents, show_progress_bar=True)

topic_model = BERTopic(
    language = "multilingual",
    embedding_model=model,
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    vectorizer_model=vectorizer,
    verbose=True
).fit(documents, document_vectors)

print("Total Number of Topics:", len(topic_model.get_topic_freq()))

topic_model.save("Nordstream_com_106_model_clussiz_800_multi")	

##Community 1029
documents = list(df1029['text'].str.lower())
document_vectors = model.encode(documents, show_progress_bar=True)

topic_model = BERTopic(
    language="multilingual",
    embedding_model=model,
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    vectorizer_model=vectorizer,
    verbose=True
).fit(documents, document_vectors)

topic_model.save("Nordstream_com_1029_model")

print("Total Number of Topics:", len(topic_model.get_topic_freq()))

##Community 246
documents = list(df246['text'].str.lower())
document_vectors = model.encode(documents, show_progress_bar=True)

topic_model = BERTopic(
    language="multilingual",
    embedding_model=model,
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    vectorizer_model=vectorizer,
    verbose=True
).fit(documents, document_vectors)

topic_model.save("Nordstream_com_246_model_clussiz_800_multi")	

print("Total Number of Topics:", len(topic_model.get_topic_freq()))

##Community 271
documents = list(df271['text'].str.lower())
document_vectors = model.encode(documents, show_progress_bar=True)

topic_model = BERTopic(
    language="multilingual",
    embedding_model=model,
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    vectorizer_model=vectorizer,
    verbose=True
).fit(documents, document_vectors)

topic_model.save("Nordstream_com_271_model_clussiz_800_multi")	

print("Total Number of Topics:", len(topic_model.get_topic_freq()))


#8.2 Compute and save models on drive for future analysis for the next 3 communities
#Given that these communities are much smaller, we decided to reduce the min_cluster_size from 800 to 300 in order to have a similar amout of topics. This choice was made after trial and error, trying out different parameters' setups to see which worked best for our case.
vectorizer = CountVectorizer(stop_words=stopwords)
umap_model = UMAP(n_neighbors=10, n_components=5, min_dist=0.0, metric='cosine', random_state=42)
hdbscan_model = HDBSCAN(min_cluster_size=300, min_samples=2, metric='euclidean', cluster_selection_method='eom')

##Community 771
documents = list(df771['text'].str.lower())
document_vectors = model.encode(documents, show_progress_bar=True)

topic_model = BERTopic(
    language="multilingual",
    embedding_model=model,
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    vectorizer_model=vectorizer,
    verbose=True
).fit(documents, document_vectors)

topic_model.save("Nordstream_com_771_model_clussiz_300_multi")	

##Community 232
hdbscan_model = HDBSCAN(min_cluster_size=300, min_samples=2, metric='euclidean', cluster_selection_method='eom')

documents = list(df232['text'].str.lower())
document_vectors = model.encode(documents, show_progress_bar=True)

topic_model = BERTopic(
    language="multilingual",
    embedding_model=model,
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    vectorizer_model=vectorizer,
    verbose=True
).fit(documents, document_vectors)

topic_model.save("Nordstream_com_232_model_clussiz_300")	

##Community 479 - here we had to increase the min_cluster_size a bit to 500
documents = list(df479['text'].str.lower())
document_vectors = model.encode(documents, show_progress_bar=True)

topic_model = BERTopic(
    language="multilingual",
    embedding_model=model,
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    vectorizer_model=vectorizer,
    verbose=True
).fit(documents, document_vectors)

topic_model.save("Nordstream_com_479_model_clussiz_500")	



